from sparv import segment, annotate

tokenizer_config = "%s/bettertokenizer.sv" % sparv_model_dir

rule _paragraph:
    """Segment paragraphs."""
    input:
        TEXT = "{file}.@TEXT",
        chunk = "{file}.%s" % paragraph_chunk
    output:
        "{file}.paragraph"
    run:
        segment.do_segmentation(
            element="p",
            out=output[0],
            text=input.TEXT,
            chunk=input.chunk,
            segmenter=paragraph_segmenter
        )

rule paragraph:
    """Expand _paragraph rule to process all files."""
    input:
        expand("%s/{file}.paragraph" % annotation_dir, file=input_files)


rule _sentence:
    """Segment sentences."""
    input:
        TEXT = "{file}.@TEXT",
        chunk = "{file}.%s" % sentence_chunk
    output:
        "{file}.sentence"
    run:
        segment.do_segmentation(
            element="s",
            out=output[0],
            text=input.TEXT,
            chunk=input.chunk,
            segmenter=sentence_segmenter
            # model="$(sentence_model)"  $(pickled_model)
        )

rule sentence:
    """Expand _sentence rule to process all files."""
    input:
        expand("%s/{file}.sentence" % annotation_dir, file=input_files)


rule _token:
    """Tokenise input."""
    input:
        TEXT = "{file}.@TEXT",
        chunk = "{file}.%s" % token_chunk
    output:
        "{file}.token"
    run:
        # Set no_pickled_model to false if default tokenizer is used
        if tokenizer_config.endswith("bettertokenizer.sv"):
            no_pickle = True
        else:
            no_pickle = False

        segment.do_segmentation(
            element="w",
            out=output[0],
            text=input.TEXT,
            chunk=input.chunk,
            segmenter=token_segmenter,
            model=tokenizer_config,
            no_pickled_model=no_pickle
        )

rule token:
    """Expand _token rule to process all files."""
    input:
        expand("%s/{file}.token" % annotation_dir, file=input_files)


rule _word:
    """Make word annotation."""
    input:
        TEXT = "{file}.@TEXT",
        token = "{file}.token"
    output:
        "{file}.word"
    run:
        annotate.text_spans(
            out=output[0],
            text=input.TEXT,
            chunk=input.token
        )

rule word:
    """Expand _word rule to process all files."""
    input:
        expand("%s/{file}.word" % annotation_dir, file=input_files)
